{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvzI0KOwAvXiWPq9bQ7YjI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/notice4/google-colab-study/blob/main/final_exam_themes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1UMfmNVeopj"
      },
      "outputs": [],
      "source": [
        "#ფინალურის საკითხები\n",
        "#ნეირონული ქსელების აგება\n",
        "#VotingClassifier  და StackingCLassifier\n",
        "#რეგრესიის  ამოცანები\n",
        "#clustering ის ამოცანები\n",
        "#GridSearchCV, PCA\n",
        "from sklearn.ensemble import VotingClassifier,StackingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "model =VotingClassifier(\n",
        "    estimators=[('boost',AdaBoostClassifier()),\n",
        "                ('Gradient',GradientBoostingClassifier()) ],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "mystacking =StackingClassifier(\n",
        "estimators=[('boost',AdaBoostClassifier()),\n",
        "                ('Gradient',GradientBoostingClassifier()) ],final_estimator=KNeighborsClassifier(),\n",
        ")\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "QUIZ #3\n",
        "\n",
        "მოცემულია ფაილი Health.csv, რომელშიც დამოკიდებული ცვლადი არის health,\n",
        "ხოლო ყველა სხვა ცვლადი არის დამოუკიდებელი ცვლადი, უპასუხეთ ქვევით\n",
        "მოცემულ კითხვებს (10 ქულა)\n",
        " ბადისებრთა ძებნის მეთოდით იპოვეთ RandomForestClassifier\n",
        "მოდელისთვის n_estimators ის ოპტიმალური მნიშვნელობები შემდეგი\n",
        "სიიდან : [25,50,75,100] max_depth ის ოპტიმალური მნიშვნელობა შემდეგი\n",
        "სიიდან [5,7,9,11] და იპოვეთ საუკეთესო score და საუკეთესო\n",
        "პარამეტრები\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/Health.csv')\n",
        "data.head()\n",
        "\n",
        "y = data['health']\n",
        "X = data.drop('health',axis=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n",
        "\n",
        "scaller = StandardScaler()\n",
        "X_train = scaller.fit_transform(X_train)\n",
        "X_test = scaller.transform(X_test)\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "param_grid = {\n",
        "    'n_estimators': [25, 50, 75, 100],\n",
        "    'max_depth': [5, 7, 9, 11]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)"
      ],
      "metadata": {
        "id": "DohaoGWWfMOL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b6af7e-76e1-4b2e-a14a-77eab7b91610"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 11, 'n_estimators': 25}\n",
            "Best Score: 0.9358823529411765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "QUIZ #4\n",
        "\n",
        "მოცემულია ფაილი heart_failure.csv, რომელშიც სვეტი DEATH_EVENT არის\n",
        "დამოკიდებული, ხოლო ყველა სხვა დანარჩენი კი დამოუკიდებელი, უპასუხეთ ქვევით\n",
        "მოცემულ კითხვებს (10 ქულა)\n",
        " Voting ის ალგორითმის მეშვეობით გააერთიანეთ შემდეგი 3 ალგორითმი :\n",
        "(GaussianNB, KneighborsClassifier, LogisticRegression) და soft ის მეთოდის\n",
        "მეშვეობით გამოთვალეთ სატესტო ქულა , სადაც სატესტო ზომა უდრის\n",
        "30%ს, შედარეთ მიღებული შედეგი ნეირონული ქსელის მოდელს,\n",
        "რომელიც შედგება 5 შრისგან და თვითოეულ შრეში არის 30 წიბო,\n",
        "აჩვენეთ რომელმა უფრო უკეთესი შედეგი აჩვენა\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from keras.src.models import Sequential\n",
        "from keras.src.layers import Dense\n",
        "\n",
        "data = pd.read_csv('/content/heart_failure (1).csv')\n",
        "data.head()\n",
        "\n",
        "y = data['DEATH_EVENT']\n",
        "X = data.drop('DEATH_EVENT',axis=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=1)\n",
        "\n",
        "scaller = StandardScaler()\n",
        "X_train = scaller.fit_transform(X_train)\n",
        "X_test = scaller.transform(X_test)\n",
        "\n",
        "model = VotingClassifier(\n",
        "    estimators=[('nb', GaussianNB()),\n",
        "                ('knn', KNeighborsClassifier()),\n",
        "                ('lr', LogisticRegression())],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Voting Classifier Accuracy:\", accuracy)\n",
        "\n",
        "nn = Sequential([\n",
        "    Dense(30, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(30, activation='relu'),\n",
        "    Dense(30, activation='relu'),\n",
        "    Dense(30, activation='relu'),\n",
        "    Dense(30, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "nn.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "nn.fit(X_train, y_train, epochs=100, batch_size=16, verbose = 0)\n",
        "loss, accuracy = nn.evaluate(X_test, y_test)\n",
        "\n",
        "print(\"Neural Network Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "513VmdKn9Ki_",
        "outputId": "058078d5-2d7f-4743-b867-c80fdc8627af"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Classifier Accuracy: 0.7888888888888889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8470 - loss: 0.3408 \n",
            "Neural Network Accuracy: 0.8111110925674438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Quiz 2\n",
        "\n",
        "შედეგი ბრძანების გამოყენებით make_classification(იხილეთ ლინკი ლინკი)\n",
        "შექმენით ხელოვნური მონაცემები თქვენი სურვილის მიხედვით(ისე რომ მთლიანი\n",
        "სვეტების რაოდენობა აღემატებიდეს 30ს და ინფორმაციული სვეტების რაოდენობა\n",
        "იყოს ნახევარზე მეტი) ,კლასების რაოდენობა იყოს 2 და ნეირონული ქსელის\n",
        "გამოყენებით განახორციელეთ კლასიფიკაციის ამოცანის\n",
        "იმპლემენტაცია.ნეირონული ქსელის მეშვეობით,უპასუხეთ შემდეგ კითხვებს (10\n",
        "ქულა)\n",
        " PCA ის მეშვეობით შეამცირეთ განზომილება თქვენს მიერ წინასწარ\n",
        "განსაზღვრულ რაოდენობამდე და ნეირონული ქსელის მოდელი აიღეთ ისე,\n",
        "რომ იგი შედგებოდეს იმდენი შრისგან, რა განზომილებამდეც შეამცირეთ\n",
        "მონაცემები, ხოლო წიბოების რაოდენობა კი PCA ის მიერ მთლიანობაში\n",
        "გამოთვლილი პროცენტულობის მიხედვით აიღეთ(გამოთვალეთ\n",
        "np.sum(explained_variance_ratio_) ის მიხედვით), რამდენი პროცენტიც უკავია\n",
        "ჯამურად, წიბოებზე იმდენი Units აიღეთ, დაატრენინგეთ მოდელი და\n",
        "გამოთვალეთ score და val_score\n",
        "\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from keras.src.models import Sequential\n",
        "from keras.src.layers import Dense\n",
        "\n",
        "X, y = make_classification(n_samples=3000, n_features=30,\n",
        "                           n_informative=20, n_redundant=5,\n",
        "                           n_classes=2, random_state=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=0.95))\n",
        "])\n",
        "\n",
        "X_train_pca = pipeline.fit_transform(X_train)\n",
        "X_test_pca = pipeline.transform(X_test)\n",
        "\n",
        "val_sum = np.sum(pipeline.named_steps['pca'].explained_variance_ratio_)\n",
        "print(\"Sum of explained variance ratio:\", val_sum)\n",
        "\n",
        "units = int(val_sum * X_train_pca.shape[1])\n",
        "print(\"Units:\", units)\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(units, activation='relu', input_shape=(X_train_pca.shape[1],)),\n",
        "    Dense(units, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train_pca, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "loss, accuracy = model.evaluate(X_test_pca, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_yjUu_dDk0q",
        "outputId": "e8e04e68-2c2e-471d-b0f7-41272244bda5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of explained variance ratio: 0.9589459129304629\n",
            "Units: 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9476 - loss: 0.1566  \n",
            "Test Accuracy: 0.9300000071525574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xl8OD3kf8csZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}